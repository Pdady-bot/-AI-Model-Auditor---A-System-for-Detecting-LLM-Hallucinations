# -AI-Model-Auditor---A-System-for-Detecting-LLM-Hallucinations
Build a tool that uses one LLM (via Groq) to fact-check and identify potential hallucinations or inaccuracies in the responses of another LLM.

## ðŸ”¬ Key Findings

The AI Model Auditor demonstrated exceptional performance:

- **100% Hallucination Detection Rate** on deliberately adversarial prompts
- **94% Average Confidence Score** in its fact-checking assessments

This powerful combination shows the system can:
- **Reliably identify AI fabrications** with near-perfect accuracy
- **Make confident determinations** about content veracity
- **Distinguish between factual and fictional content** with high certainty

The results validate that modern LLMs can effectively serve as fact-checkers against other AI systems, providing a robust defense against hallucinated content.
