# -AI-Model-Auditor---A-System-for-Detecting-LLM-Hallucinations
Build a tool that uses one LLM (via Groq) to fact-check and identify potential hallucinations or inaccuracies in the responses of another LLM.
